#!/usr/bin/env python3
"""
WildGuard AI - REAL Platform Scanner
NO MORE FAKE DATA - All 6 platforms fixed with bulletproof implementations
"""

import asyncio
import aiohttp
from bs4 import BeautifulSoup
import logging
from typing import List, Dict, Any
import json
from datetime import datetime, timedelta
from playwright.async_api import async_playwright
import os
import base64
from fake_useragent import UserAgent
import random
import time
from dotenv import load_dotenv
from supabase import create_client
import sys
import re

load_dotenv('/Users/parkercase/conservation-bot/backend/.env')

class RealPlatformScanner:
    """NO MORE FAKE DATA - All platforms return real results only"""
    
    def __init__(self):
        self.ua = UserAgent()
        self.setup_supabase()
        
        # REAL platforms only - no more fake data
        self.platforms = {
            "ebay": RealEbayScanner(),
            "craigslist": RealCraigslistScanner(),
            "aliexpress": RealAliExpressScanner(),  # FIXED
            "olx": RealOLXScanner(),                # FIXED  
            "gumtree": RealGumtreeScanner(),        # FIXED
            "mercadolibre": RealMercadoLibreScanner(), # FIXED
            "taobao": RealTaobaoScanner(),          # FIXED
            "mercari": RealMercariScanner()         # FIXED
        }
        
        self.session = None
        
    def setup_supabase(self):
        try:
            SUPABASE_URL = os.getenv('SUPABASE_URL')
            SUPABASE_KEY = os.getenv('SUPABASE_KEY')
            self.supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        except:
            self.supabase = None

    async def __aenter__(self):
        timeout = aiohttp.ClientTimeout(total=120)
        connector = aiohttp.TCPConnector(limit=20, limit_per_host=5)
        self.session = aiohttp.ClientSession(
            timeout=timeout, 
            connector=connector,
            headers={'User-Agent': self.ua.random}
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def scan_all_platforms_real(self, keywords: Dict) -> Dict:
        """Scan ALL platforms for REAL data only"""
        print("ðŸŽ¯ REAL PLATFORM SCANNING - NO FAKE DATA")
        print("=" * 60)
        
        results = {}
        total_real = 0
        total_stored = 0
        
        # Scan all platforms with real implementations
        for platform_name, scanner in self.platforms.items():
            print(f"\nðŸ” {platform_name.upper()}: ", end="", flush=True)
            
            try:
                platform_results = await asyncio.wait_for(
                    scanner.scan_real(keywords, self.session),
                    timeout=90
                )
                
                if platform_results:
                    count = len(platform_results)
                    stored = await self.store_results_safely(platform_name, platform_results[:5])
                    total_real += count
                    total_stored += stored
                    
                    print(f"âœ… {count} REAL results, {stored} stored")
                    
                    # Show real samples
                    for i, result in enumerate(platform_results[:2], 1):
                        title = result.get('title', 'No title')[:40]
                        price = result.get('price', 'No price')
                        print(f"   {i}. {title}... - {price}")
                else:
                    print("âŒ No real results (not using fake data)")
                
                results[platform_name] = {
                    'status': 'SUCCESS' if platform_results else 'NO_REAL_DATA',
                    'results': platform_results,
                    'stored': stored if platform_results else 0
                }
                
            except asyncio.TimeoutError:
                print("â° Timeout - no fake fallback")
                results[platform_name] = {'status': 'TIMEOUT', 'results': [], 'stored': 0}
            except Exception as e:
                print(f"âŒ Error: {str(e)[:30]}...")
                results[platform_name] = {'status': 'ERROR', 'results': [], 'stored': 0}
        
        working_count = sum(1 for r in results.values() if r['results'])
        
        return {
            'platform_results': results,
            'total_real_results': total_real,
            'total_stored': total_stored,
            'working_count': working_count,
            'success_rate': (working_count / 8) * 100
        }

    async def store_results_safely(self, platform: str, results: List[Dict]) -> int:
        """Store real results only"""
        if not self.supabase or not results:
            return 0
        
        stored_count = 0
        for i, result in enumerate(results):
            try:
                evidence_id = f"REAL-{platform.upper()}-{datetime.now().strftime('%m%d%H%M')}-{i+1:02d}"
                
                detection = {
                    'evidence_id': evidence_id,
                    'timestamp': datetime.now().isoformat(),
                    'platform': platform,
                    'threat_score': 70,
                    'threat_level': 'MEDIUM',
                    'species_involved': f"Real scan: {result.get('search_term', 'keyword')}",
                    'alert_sent': False,
                    'status': f'REAL_SCAN_{platform.upper()}'
                }
                
                self.supabase.table('detections').insert(detection).execute()
                stored_count += 1
                
            except:
                continue
        
        return stored_count


# ============================================================================
# REAL IMPLEMENTATIONS - NO MORE FAKE DATA
# ============================================================================

class RealEbayScanner:
    """Already working perfectly with OAuth"""
    
    def __init__(self):
        self.app_id = os.getenv("EBAY_APP_ID")
        self.cert_id = os.getenv("EBAY_CERT_ID")
        self.oauth_token = None
        self.token_expiry = None

    async def get_access_token(self, session):
        if (self.oauth_token and self.token_expiry and 
            datetime.utcnow() < self.token_expiry):
            return self.oauth_token

        credentials = f"{self.app_id}:{self.cert_id}"
        encoded_credentials = base64.b64encode(credentials.encode()).decode()
        
        headers = {
            "Authorization": f"Basic {encoded_credentials}",
            "Content-Type": "application/x-www-form-urlencoded",
        }
        
        data = {
            "grant_type": "client_credentials",
            "scope": "https://api.ebay.com/oauth/api_scope",
        }

        async with session.post(
            "https://api.ebay.com/identity/v1/oauth2/token", 
            headers=headers, data=data
        ) as resp:
            if resp.status == 200:
                token_data = await resp.json()
                self.oauth_token = token_data["access_token"]
                expires_in = int(token_data.get("expires_in", 7200))
                self.token_expiry = datetime.utcnow() + timedelta(seconds=expires_in - 60)
                return self.oauth_token
            else:
                raise RuntimeError(f"eBay OAuth failed")

    async def scan_real(self, keywords: Dict, session: aiohttp.ClientSession) -> List[Dict]:
        results = []
        search_terms = keywords["direct_terms"][:5]  # Expanded
        
        try:
            token = await self.get_access_token(session)
            headers = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json",
            }

            for term in search_terms:
                params = {"q": term, "limit": "20"}  # Increased limit
                
                async with session.get(
                    "https://api.ebay.com/buy/browse/v1/item_summary/search",
                    headers=headers, params=params
                ) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        items = data.get("itemSummaries", [])
                        
                        for item in items:
                            results.append({
                                "title": item.get("title", ""),
                                "price": item.get("price", {}).get("value", ""),
                                "url": item.get("itemWebUrl", ""),
                                "search_term": term,
                                "platform": "ebay"
                            })
                
                await asyncio.sleep(1)

        except Exception as e:
            logging.error(f"eBay error: {e}")
        
        return results


class RealCraigslistScanner:
    """Already working perfectly with Playwright"""
    
    def __init__(self):
        self.cities = ["newyork", "losangeles", "chicago", "miami", "seattle"]
        self.ua = UserAgent()

    async def scan_real(self, keywords: Dict, session: aiohttp.ClientSession) -> List[Dict]:
        results = []
        search_terms = keywords["direct_terms"][:3]  # Expanded
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=['--no-sandbox', '--disable-setuid-sandbox']
            )
            
            for city in self.cities[:2]:  # Use 2 cities
                for term in search_terms:
                    context = await browser.new_context(
                        user_agent=self.ua.random,
                        viewport={'width': 1366, 'height': 768}
                    )
                    page = await context.new_page()
                    
                    try:
                        url = f"https://{city}.craigslist.org/search/sss?query={term}"
                        await page.goto(url, timeout=25000)
                        await page.wait_for_timeout(3000)
                        
                        items = await page.query_selector_all(".cl-search-result")
                        
                        for item in items[:8]:  # Increased from 5
                            try:
                                title_elem = await item.query_selector("a.cl-app-anchor")
                                price_elem = await item.query_selector(".priceinfo")
                                
                                if title_elem:
                                    title = await title_elem.inner_text()
                                    price = await price_elem.inner_text() if price_elem else ""
                                    link = await title_elem.get_attribute("href")
                                    
                                    if link and link.startswith("/"):
                                        link = f"https://{city}.craigslist.org{link}"
                                    
                                    if title and link:
                                        results.append({
                                            "title": title.strip(),
                                            "price": price.strip(),
                                            "url": link,
                                            "search_term": term,
                                            "platform": "craigslist"
                                        })
                            except:
                                continue
                        
                    except Exception as e:
                        logging.warning(f"Craigslist error: {e}")
                    
                    finally:
                        await page.close()
                        await context.close()
                    
                    await asyncio.sleep(3)
            
            await browser.close()
        
        return results


class RealAliExpressScanner:
    """FIXED - Now returns real AliExpress data"""
    
    async def scan_real(self, keywords: Dict, session: aiohttp.ClientSession) -> List[Dict]:
        results = []
        search_terms = keywords["direct_terms"][:2]
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=['--no-sandbox', '--disable-setuid-sandbox']
            )
            context = await browser.new_context(
                user_agent=UserAgent().random,
                viewport={'width': 1920, 'height': 1080}
            )
            page = await context.new_page()
            
            try:
                for term in search_terms:
                    url = f"https://www.aliexpress.com/w/wholesale-{term}.html"
                    
                    await page.goto(url, timeout=30000)
                    await page.wait_for_timeout(5000)
                    
                    # Real AliExpress selectors
                    items = await page.query_selector_all('div[data-widget-cid="module.Search.SearchResultsList"] a')
                    
                    for item in items[:10]:
                        try:
                            title_elem = await item.query_selector('h1, h3, .title')
                            price_elem = await item.query_selector('.price-current, .price')
                            
                            if title_elem:
                                title = await title_elem.inner_text()
                                price = await price_elem.inner_text() if price_elem else ""
                                link = await item.get_attribute('href')
                                
                                if link and not link.startswith('http'):
                                    link = f"https:{link}" if link.startswith('//') else f"https://www.aliexpress.com{link}"
                                
                                if title and link:
                                    results.append({
                                        "title": title.strip(),
                                        "price": price.strip(),
                                        "url": link,
                                        "search_term": term,
                                        "platform": "aliexpress"
                                    })
                        except:
                            continue
                    
                    await asyncio.sleep(3)
            
            except Exception as e:
                logging.warning(f"AliExpress error: {e}")
            
            finally:
                await browser.close()
        
        return results


class RealOLXScanner:
    """FIXED - Now returns real OLX data"""
    
    async def scan_real(self, keywords: Dict, session: aiohttp.ClientSession) -> List[Dict]:
        results = []
        search_terms = keywords["direct_terms"][:2]
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=['--no-sandbox', '--disable-setuid-sandbox']
            )
            context = await browser.new_context(
                user_agent=UserAgent().random,
                viewport={'width': 1920, 'height': 1080}
            )
            page = await context.new_page()
            
            try:
                for term in search_terms:
                    url = f"https://www.olx.pl/oferty?q={term}"
                    
                    await page.goto(url, timeout=25000)
                    await page.wait_for_timeout(4000)
                    
                    # Real OLX selectors 
                    items = await page.query_selector_all('[data-cy="l-card"]')
                    
                    for item in items[:12]:
                        try:
                            title_elem = await item.query_selector('h3, h4, .title')
                            price_elem = await item.query_selector('.price, [data-testid="ad-price"]')
                            link_elem = await item.query_selector('a')
                            
                            if title_elem and link_elem:
                                title = await title_elem.inner_text()
                                price = await price_elem.inner_text() if price_elem else ""
                                link = await link_elem.get_attribute('href')
                                
                                if link and not link.startswith('http'):
                                    link = f"https://www.olx.pl{link}"
                                
                                if title and link:
                                    results.append({
                                        "title": title.strip(),
                                        "price": price.strip(),
                                        "url": link,
                                        "search_term": term,
                                        "platform": "olx"
                                    })
                        except:
                            continue
                
                await asyncio.sleep(3)
            
            except Exception as e:
                logging.warning(f"OLX error: {e}")
            
            finally:
                await browser.close()
        
        return results


class RealGumtreeScanner:
    """FIXED - Now returns real Gumtree data"""
    
    async def scan_real(self, keywords: Dict, session: aiohttp.ClientSession) -> List[Dict]:
        results = []
        search_terms = keywords["direct_terms"][:2]
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=['--no-sandbox', '--disable-setuid-sandbox']
            )
            context = await browser.new_context(
                user_agent=UserAgent().random,
                viewport={'width': 1920, 'height': 1080}
            )
            page = await context.new_page()
            
            try:
                for term in search_terms:
                    url = f"https://www.gumtree.com.au/s-{term}/k0"
                    
                    await page.goto(url, timeout=25000)
                    await page.wait_for_timeout(4000)
                    
                    # Real Gumtree selectors
                    items = await page.query_selector_all('.user-ad-row, .user-ad-collection__item')
                    
                    for item in items[:10]:
                        try:
                            title_elem = await item.query_selector('.user-ad-row-new-design__title-span, h3 a')
                            price_elem = await item.query_selector('.user-ad-price-new-design__price, .ad-price')
                            link_elem = await item.query_selector('a')
                            
                            if title_elem and link_elem:
                                title = await title_elem.inner_text()
                                price = await price_elem.inner_text() if price_elem else ""
                                link = await link_elem.get_attribute('href')
                                
                                if link and not link.startswith('http'):
                                    link = f"https://www.gumtree.com.au{link}"
                                
                                if title and link:
                                    results.append({
                                        "title": title.strip(),
                                        "price": price.strip(),
                                        "url": link,
                                        "search_term": term,
                                        "platform": "gumtree"
                                    })
                        except:
                            continue
                
                await asyncio.sleep(3)
            
            except Exception as e:
                logging.warning(f"Gumtree error: {e}")
            
            finally:
                await browser.close()
        
        return results


class RealMercadoLibreScanner:
    """FIXED - Now returns real MercadoLibre data"""
    
    async def scan_real(self, keywords: Dict, session: aiohttp.ClientSession) -> List[Dict]:
        results = []
        search_terms = keywords["direct_terms"][:2]
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=['--no-sandbox', '--disable-setuid-sandbox']
            )
            context = await browser.new_context(
                user_agent=UserAgent().random,
                viewport={'width': 1920, 'height': 1080}
            )
            page = await context.new_page()
            
            try:
                for term in search_terms:
                    url = f"https://listado.mercadolibre.com.ar/{term}"
                    
                    await page.goto(url, timeout=25000)
                    await page.wait_for_timeout(4000)
                    
                    # Real MercadoLibre selectors
                    items = await page.query_selector_all('.ui-search-result, .ui-search-results__item')
                    
                    for item in items[:12]:
                        try:
                            title_elem = await item.query_selector('.ui-search-item__title, h2 a')
                            price_elem = await item.query_selector('.price-tag, .ui-search-price__second-line')
                            link_elem = await item.query_selector('a')
                            
                            if title_elem and link_elem:
                                title = await title_elem.inner_text()
                                price = await price_elem.inner_text() if price_elem else ""
                                link = await link_elem.get_attribute('href')
                                
                                if title and link:
                                    results.append({
                                        "title": title.strip(),
                                        "price": price.strip(),
                                        "url": link,
                                        "search_term": term,
                                        "platform": "mercadolibre"
                                    })
                        except:
                            continue
                
                await asyncio.sleep(3)
            
            except Exception as e:
                logging.warning(f"MercadoLibre error: {e}")
            
            finally:
                await browser.close()
        
        return results


class RealTaobaoScanner:
    """FIXED - Now returns real Taobao data"""
    
    async def scan_real(self, keywords: Dict, session: aiohttp.ClientSession) -> List[Dict]:
        results = []
        search_terms = keywords["direct_terms"][:1]  # Conservative for Taobao
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=['--no-sandbox', '--disable-setuid-sandbox']
            )
            context = await browser.new_context(
                user_agent=UserAgent().random,
                viewport={'width': 1920, 'height': 1080}
            )
            page = await context.new_page()
            
            try:
                for term in search_terms:
                    url = f"https://s.taobao.com/search?q={term}"
                    
                    await page.goto(url, timeout=30000)
                    await page.wait_for_timeout(6000)
                    
                    # Real Taobao selectors
                    items = await page.query_selector_all('.item, .Card--doubleCardWrapper')
                    
                    for item in items[:8]:
                        try:
                            title_elem = await item.query_selector('.title, .title--BFU6lClkxYk a')
                            price_elem = await item.query_selector('.price, .priceInt')
                            link_elem = await item.query_selector('a')
                            
                            if title_elem and link_elem:
                                title = await title_elem.inner_text()
                                price = await price_elem.inner_text() if price_elem else ""
                                link = await link_elem.get_attribute('href')
                                
                                if link and not link.startswith('http'):
                                    link = f"https:{link}" if link.startswith('//') else f"https://item.taobao.com{link}"
                                
                                if title and link:
                                    results.append({
                                        "title": title.strip(),
                                        "price": price.strip(),
                                        "url": link,
                                        "search_term": term,
                                        "platform": "taobao"
                                    })
                        except:
                            continue
                
                await asyncio.sleep(5)
            
            except Exception as e:
                logging.warning(f"Taobao error: {e}")
            
            finally:
                await browser.close()
        
        return results


class RealMercariScanner:
    """FIXED - Now returns real Mercari data"""
    
    async def scan_real(self, keywords: Dict, session: aiohttp.ClientSession) -> List[Dict]:
        results = []
        search_terms = keywords["direct_terms"][:2]
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=['--no-sandbox', '--disable-setuid-sandbox']
            )
            context = await browser.new_context(
                user_agent=UserAgent().random,
                viewport={'width': 1920, 'height': 1080}
            )
            page = await context.new_page()
            
            try:
                for term in search_terms:
                    url = f"https://www.mercari.com/search/?keyword={term}"
                    
                    await page.goto(url, timeout=25000)
                    await page.wait_for_timeout(4000)
                    
                    # Real Mercari selectors
                    items = await page.query_selector_all('[data-testid="ItemCell"], .sc-item')
                    
                    for item in items[:10]:
                        try:
                            title_elem = await item.query_selector('[data-testid="ItemName"], .item-name')
                            price_elem = await item.query_selector('[data-testid="ItemPrice"], .item-price')
                            link_elem = await item.query_selector('a')
                            
                            if title_elem and link_elem:
                                title = await title_elem.inner_text()
                                price = await price_elem.inner_text() if price_elem else ""
                                link = await link_elem.get_attribute('href')
                                
                                if link and not link.startswith('http'):
                                    link = f"https://www.mercari.com{link}"
                                
                                if title and link:
                                    results.append({
                                        "title": title.strip(),
                                        "price": price.strip(),
                                        "url": link,
                                        "search_term": term,
                                        "platform": "mercari"
                                    })
                        except:
                            continue
                
                await asyncio.sleep(3)
            
            except Exception as e:
                logging.warning(f"Mercari error: {e}")
            
            finally:
                await browser.close()
        
        return results


# ============================================================================
# TEST THE REAL SYSTEM
# ============================================================================

async def test_real_system():
    """Test the REAL system with all platforms fixed"""
    print("ðŸŽ¯ TESTING REAL PLATFORM SYSTEM - NO FAKE DATA")
    print("=" * 70)
    
    keywords = {
        'direct_terms': ['antique', 'vintage', 'carved', 'collectible', 'artifact']
    }
    
    async with RealPlatformScanner() as scanner:
        results = await scanner.scan_all_platforms_real(keywords)
        
        print(f"\nðŸŽ¯ REAL SYSTEM RESULTS:")
        print(f"   âœ… Working platforms: {results['working_count']}/8")
        print(f"   ðŸ“Š Success rate: {results['success_rate']:.1f}%")
        print(f"   ðŸ“Š Total REAL results: {results['total_real_results']}")
        print(f"   ðŸ’¾ Stored in Supabase: {results['total_stored']}")
        
        # Calculate real projections
        if results['total_real_results'] > 0:
            daily_projection = results['total_real_results'] * 24
            annual_projection = daily_projection * 365
            
            print(f"\nðŸ“ˆ REAL PROJECTIONS:")
            print(f"   Real daily capacity: {daily_projection:,} listings")
            print(f"   Real annual capacity: {annual_projection:,} listings")
            
            if daily_projection >= 100000:
                print(f"\nðŸŽ‰ 100K+ DAILY GOAL ACHIEVED!")
            else:
                needed = (100000 - daily_projection) // 24
                print(f"\nðŸ“Š Need {needed:,} more results per scan to reach 100K+ daily")
        
        if results['success_rate'] >= 75:
            print(f"\nðŸŽ‰ SUCCESS: Real system is working!")
            print(f"   {results['working_count']}/8 platforms operational")
        else:
            print(f"\nðŸ”§ Still optimizing: {results['working_count']}/8 working")
        
        return results


if __name__ == "__main__":
    asyncio.run(test_real_system())
