import anthropic\nimport json\nimport logging\nfrom typing import Dict, List, Any\nimport asyncio\nfrom datetime import datetime\nimport httpx\nimport base64\nimport os\nimport hashlib\nimport redis\nfrom collections import defaultdict\n\n\nclass CostOptimizedThreatAnalyzer:\n    \"\"\"Cost-optimized threat analyzer for 545,940+ listings\"\"\"\n    \n    def __init__(self, api_key: str, daily_budget: float = 50.0):\n        self.client = anthropic.Anthropic(api_key=api_key)\n        self.daily_budget = daily_budget\n        self.cost_per_token = {\n            'haiku': 0.00000025,    # $0.25/1M input tokens\n            'sonnet': 0.000003,     # $3/1M input tokens  \n            'opus': 0.000015        # $15/1M input tokens\n        }\n        self.daily_spent = 0.0\n        self.threat_patterns = self._load_threat_patterns()\n        \n        # Try to connect to Redis for caching (optional)\n        try:\n            self.redis = redis.Redis(host='localhost', port=6379, db=0)\n            self.redis.ping()\n        except:\n            self.redis = None\n            logging.warning(\"Redis not available - caching disabled\")\n    \n    def calculate_daily_limit(self) -> int:\n        \"\"\"Calculate how many API calls we can afford today\"\"\"\n        avg_cost_per_call = 0.003  # Using Haiku for most calls\n        return int((self.daily_budget - self.daily_spent) / avg_cost_per_call)\n    \n    def pre_filter_listing(self, listing: Dict) -> float:\n        \"\"\"Free pre-filtering to reduce AI API calls by ~90%\"\"\"\n        \n        title = listing.get('title', '').lower()\n        description = listing.get('description', '').lower()\n        price = float(listing.get('price', 0))\n        platform = listing.get('platform', '').lower()\n        \n        # High-risk keywords (your existing comprehensive list)\n        high_risk_keywords = [\n            'ivory', 'rhino horn', 'tiger bone', 'pangolin', 'bear bile',\n            'elephant tusk', 'leopard skin', 'jaguar pelt', 'cheetah fur',\n            'traditional medicine', 'tcm', 'rare specimen', 'museum quality'\n        ]\n        \n        # Calculate base threat score (0-100)\n        keyword_score = sum(10 for keyword in high_risk_keywords if keyword in title + description)\n        \n        # Price-based scoring\n        price_score = 0\n        if price > 1000: price_score = 30      # Very expensive items\n        elif price > 100: price_score = 15     # Moderately expensive\n        elif price > 10: price_score = 5       # Reasonable price\n        \n        # Platform-based multiplier\n        high_risk_platforms = ['craigslist', 'facebook', 'olx']\n        platform_multiplier = 1.5 if platform in high_risk_platforms else 1.0\n        \n        # Suspicious phrases\n        suspicious_phrases = ['cash only', 'no questions', 'discreet', 'private sale']\n        suspicious_score = sum(5 for phrase in suspicious_phrases if phrase in description)\n        \n        total_score = (keyword_score + price_score + suspicious_score) * platform_multiplier\n        \n        return min(total_score, 100)  # Cap at 100\n    \n    async def get_cached_analysis(self, listing: Dict) -> Dict:\n        \"\"\"Check if we've already analyzed similar content\"\"\"\n        if not self.redis:\n            return None\n            \n        # Create content hash for caching\n        content = f\"{listing.get('title', '')}|{listing.get('description', '')[:200]}\"\n        content_hash = hashlib.md5(content.encode()).hexdigest()\n        cache_key = f\"threat_analysis:{content_hash}\"\n        \n        try:\n            cached = self.redis.get(cache_key)\n            if cached:\n                result = json.loads(cached)\n                result['cached'] = True\n                return result\n        except Exception as e:\n            logging.warning(f\"Cache read failed: {e}\")\n        \n        return None\n    \n    async def cache_analysis(self, listing: Dict, result: Dict):\n        \"\"\"Cache analysis result for 24 hours\"\"\"\n        if not self.redis:\n            return\n            \n        content = f\"{listing.get('title', '')}|{listing.get('description', '')[:200]}\"\n        content_hash = hashlib.md5(content.encode()).hexdigest()\n        cache_key = f\"threat_analysis:{content_hash}\"\n        \n        try:\n            self.redis.setex(cache_key, 86400, json.dumps(result))  # 24h cache\n        except Exception as e:\n            logging.warning(f\"Cache write failed: {e}\")\n    \n    async def analyze_listing_batch(self, listings: List[Dict], model: str = \"claude-3-haiku-20240307\") -> List[Dict]:\n        \"\"\"Analyze multiple listings in a single API call for cost efficiency\"\"\"\n        \n        if len(listings) == 0:\n            return []\n            \n        # Prepare batch prompt\n        batch_data = []\n        for i, listing in enumerate(listings):\n            batch_data.append({\n                'id': i,\n                'title': listing.get('title', '')[:100],  # Truncate to save tokens\n                'description': listing.get('description', '')[:200],\n                'price': listing.get('price', 0),\n                'platform': listing.get('platform', '')\n            })\n        \n        prompt = f\"\"\"\nAnalyze these {len(listings)} marketplace listings for illegal wildlife trafficking indicators.\nFor each listing, provide threat_score (0-100), threat_level (LOW/MEDIUM/HIGH), and confidence (0-1).\n\nListings:\n{json.dumps(batch_data, indent=1)}\n\nRespond with JSON array: [{\"id\": 0, \"threat_score\": 85, \"threat_level\": \"HIGH\", \"confidence\": 0.9, \"reasoning\": \"Contains rhino horn keywords and suspicious pricing\"}, ...]\n\"\"\"\n        \n        try:\n            response = self.client.messages.create(\n                model=model,\n                max_tokens=1000 + (len(listings) * 50),  # Scale tokens with batch size\n                messages=[{\"role\": \"user\", \"content\": prompt}]\n            )\n            \n            # Estimate cost\n            estimated_tokens = len(prompt) + (len(listings) * 50)\n            cost = estimated_tokens * self.cost_per_token.get(model.split('-')[2], 0.000003)\n            self.daily_spent += cost\n            \n            # Parse response\n            response_text = response.content[0].text\n            \n            # Extract JSON from response\n            start = response_text.find('[')\n            end = response_text.rfind(']') + 1\n            if start != -1 and end != -1:\n                results = json.loads(response_text[start:end])\n                \n                # Map results back to listings\n                final_results = []\n                for i, listing in enumerate(listings):\n                    if i < len(results):\n                        result = results[i]\n                        result.update({\n                            'listing_id': listing.get('id', ''),\n                            'analysis_timestamp': datetime.utcnow().isoformat(),\n                            'model_used': model,\n                            'cost': cost / len(listings)\n                        })\n                        final_results.append(result)\n                        \n                        # Cache individual results\n                        await self.cache_analysis(listing, result)\n                \n                return final_results\n                \n        except Exception as e:\n            logging.error(f\"Batch analysis failed: {e}\")\n            \n        # Fallback: return basic scores\n        return [{\n            'threat_score': self.pre_filter_listing(listing),\n            'threat_level': 'MEDIUM',\n            'confidence': 0.5,\n            'error': 'AI analysis failed - using rule-based scoring',\n            'listing_id': listing.get('id', ''),\n            'analysis_timestamp': datetime.utcnow().isoformat()\n        } for listing in listings]\n    \n    async def analyze_listing_smart(self, listing: Dict) -> Dict:\n        \"\"\"Smart analysis with cost optimization and caching\"\"\"\n        \n        # Step 1: Check cache first\n        cached_result = await self.get_cached_analysis(listing)\n        if cached_result:\n            return cached_result\n        \n        # Step 2: Pre-filter to avoid unnecessary AI calls\n        pre_score = self.pre_filter_listing(listing)\n        \n        # Step 3: If low pre-score, use rule-based analysis\n        if pre_score < 20:\n            result = {\n                'threat_score': pre_score,\n                'threat_level': 'LOW' if pre_score < 30 else 'MEDIUM',\n                'confidence': 0.7,\n                'reasoning': 'Rule-based analysis - low threat indicators',\n                'listing_id': listing.get('id', ''),\n                'analysis_timestamp': datetime.utcnow().isoformat(),\n                'method': 'rule_based'\n            }\n            await self.cache_analysis(listing, result)\n            return result\n        \n        # Step 4: Check daily budget\n        if self.daily_spent >= self.daily_budget:\n            result = {\n                'threat_score': pre_score,\n                'threat_level': 'MEDIUM',\n                'confidence': 0.6,\n                'reasoning': 'Daily budget exceeded - using pre-filter score',\n                'listing_id': listing.get('id', ''),\n                'analysis_timestamp': datetime.utcnow().isoformat(),\n                'method': 'budget_limited'\n            }\n            await self.cache_analysis(listing, result)\n            return result\n        \n        # Step 5: Choose AI model based on priority\n        model = \"claude-3-haiku-20240307\"  # Default to cheapest\n        if pre_score > 70 and listing.get('price', 0) > 100:\n            model = \"claude-3-sonnet-20240229\"  # Use better model for high-priority\n        \n        # Step 6: AI analysis\n        results = await self.analyze_listing_batch([listing], model)\n        return results[0] if results else {'error': 'Analysis failed'}\n    \n    async def process_daily_batch(self, listings: List[Dict], max_budget: float = None) -> List[Dict]:\n        \"\"\"Process a batch of listings within daily budget constraints\"\"\"\n        \n        if max_budget:\n            self.daily_budget = max_budget\n            self.daily_spent = 0.0\n        \n        # Sort by pre-filter score (highest priority first)\n        listings_with_scores = [(listing, self.pre_filter_listing(listing)) for listing in listings]\n        listings_with_scores.sort(key=lambda x: x[1], reverse=True)\n        \n        results = []\n        batch_size = 20  # Process in batches of 20\n        \n        for i in range(0, len(listings_with_scores), batch_size):\n            if self.daily_spent >= self.daily_budget:\n                logging.info(f\"Daily budget of ${self.daily_budget} reached. Processed {len(results)} listings.\")\n                break\n                \n            batch = [item[0] for item in listings_with_scores[i:i+batch_size]]\n            \n            # Check cache for each listing first\n            uncached_batch = []\n            for listing in batch:\n                cached = await self.get_cached_analysis(listing)\n                if cached:\n                    results.append(cached)\n                else:\n                    uncached_batch.append(listing)\n            \n            # Process uncached listings\n            if uncached_batch:\n                batch_results = await self.analyze_listing_batch(uncached_batch)\n                results.extend(batch_results)\n        \n        logging.info(f\"Processed {len(results)} listings. Daily spent: ${self.daily_spent:.2f}\")\n        return results\n    \n    def _load_threat_patterns(self) -> Dict:\n        \"\"\"Load comprehensive threat patterns\"\"\"\n        return {\n            \"high_risk_species\": [\n                \"elephant\", \"rhino\", \"tiger\", \"pangolin\", \"bear\", \"shark\", \n                \"turtle\", \"leopard\", \"jaguar\", \"cheetah\", \"lion\", \"wolf\"\n            ],\n            \"risk_multipliers\": {\n                \"cash_only\": 2.0,\n                \"discreet_shipping\": 1.8, \n                \"no_questions\": 2.5,\n                \"traditional_medicine\": 1.5,\n                \"tcm\": 1.5,\n                \"rare_specimen\": 2.0\n            }\n        }\n\n\n# Usage example:\nif __name__ == \"__main__\":\n    async def main():\n        analyzer = CostOptimizedThreatAnalyzer(\n            api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n            daily_budget=50.0  # $50/day budget\n        )\n        \n        # Example: Process 1000 listings within budget\n        sample_listings = [\n            {\"id\": \"1\", \"title\": \"Vintage ivory carving\", \"price\": 500, \"platform\": \"ebay\"},\n            {\"id\": \"2\", \"title\": \"Traditional medicine herbs\", \"price\": 50, \"platform\": \"craigslist\"},\n            # ... more listings\n        ]\n        \n        results = await analyzer.process_daily_batch(sample_listings)\n        \n        print(f\"Analyzed {len(results)} listings\")\n        print(f\"Daily spent: ${analyzer.daily_spent:.2f}\")\n        \n        # High-threat detections\n        high_threats = [r for r in results if r.get('threat_score', 0) > 70]\n        print(f\"High-threat detections: {len(high_threats)}\")\n    \n    asyncio.run(main())\n